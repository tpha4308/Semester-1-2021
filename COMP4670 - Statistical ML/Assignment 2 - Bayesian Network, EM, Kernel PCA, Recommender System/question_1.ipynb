{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Union, Dict\n",
    "from scipy.special import logsumexp\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Networks and EM\n",
    "\n",
    "In the following questions we will be considering a Bayesian Network for modelling variables related to students' ability to get a job (note this is a toy example aiming to encode a few simple intuitions, the values are randomly made).\n",
    "\n",
    "It has the following variables:\n",
    "- Difficulty (D) of a job-related course in the year the student took the course\n",
    "- Effort (E) that the student put into the course and then finding a job\n",
    "- Aptitude (A) of the student\n",
    "- Confidence (C) of the student in the related subject\n",
    "- Grade (G) of the student in the course\n",
    "- Interview (I) that the student took for the job\n",
    "- TuteAttendance (T) of the student in the course\n",
    "- ForumParticipation (F) of the student in the course\n",
    "- SAT (S) that the student got (Scholarly Aptitute Test, a rough measure of Aptitude)\n",
    "- Job (J) - whether the student got the job or not\n",
    "\n",
    "These variables will be the nodes of our Bayesian Network. We will specify how we believe the variables are affected by each other by the structure of the graph, for example Grade's parents are Difficulty, Effort and Aptitude as the values for those variables directly affect the value of Grade.\n",
    "\n",
    "To represent the Bayesian network, we first define two classes, one for Probability Mass Function objects (`PMF`) that have an array and what each axis means, and the second is a general Bayesian Network node (`BNNode`) that has the nodes' state values, its parents, and its pmf (an object on type `PMF`).\n",
    "\n",
    "The PMF array is always meant to be a conditional probability table, with the probabilities given for the last column given the previous columns. For example `PMF(['Difficulty', 'Effort', 'Aptitude', 'Grade'], (3, 3, 5, 5))` represents $p(G|D,E,A)$, and $p(G=g|D=d,E=e,A=a)$ is at index `[d,e,a,g]` of the array.\n",
    "\n",
    "You should read through the definition of these classes, as you will be working with these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "class PMF:\n",
    "    def __init__(self, dims: List['BNNode'], values: np.array):\n",
    "        self.dims = dims\n",
    "        self.values = values\n",
    "    def __repr__(self):\n",
    "        return \"PMF({}, {})\".format(vars2names(self.dims), self.values.shape)\n",
    "\n",
    "class BNNode:\n",
    "    def __init__(self, name, states: List, parents: [List['BNNode']], pmf):\n",
    "        self.name = name\n",
    "        self.states = states\n",
    "        self.num_states = len(self.states)\n",
    "        self.parents = parents\n",
    "        self.pmf = pmf # {'dim_vars': [names], 'value':pmf} where pmf is (num_states,) or (parents1.num_states,...,num_states)\n",
    "    def check_pmf(self, pmf):\n",
    "        assert type(pmf) == PMF, \"Expect pmf to be of type PMF, got {}\".format(type(pmf))\n",
    "        assert len(pmf.dims)>0, \"Expect at least 1 dim, got {} dims\".format(len(pmf.dims))\n",
    "        assert self == pmf.dims[-1], \"Last dim should be {}, got {}\".format(self.name, pmf.dims[-1].name)\n",
    "        # Make sure the pmf is of the correct shape\n",
    "        expected_shapes = tuple(var.num_states for var in pmf.dims)\n",
    "        assert pmf.values.shape == expected_shapes,  \\\n",
    "            \"Got shape {} for pmf, expected {}\".format(pmf.values.shape, expected_shapes)\n",
    "        assert np.allclose(pmf.values.sum(axis=-1), 1), \"Not all last dims sum to 1: sums: {}\".format(pmf.values.sum(axis=-1))\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Define equality of nodes if they have the same name\"\"\"\n",
    "        if isinstance(other, BNNode):\n",
    "            return self.name == other.name\n",
    "        return False\n",
    "    def __hash__(self):\n",
    "        return hash(self.name)\n",
    "    def __repr__(self):\n",
    "        return \"BNNode[{}]\".format(self.name)\n",
    "        \n",
    "# Useful for debugging!\n",
    "vars2names = lambda var_lst: [var.name for var in var_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will actually define a specific Bayesian Network for our problem instance using our BN node. Note this has defined the parents of each node, which defines the structue of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Difficulty', 'Effort', 'Aptitute', 'Confidence', 'Grade', 'Interview', 'TuteAttendence', 'ForumParticipation', 'SAT', 'Job']\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "D = BNNode(\"Difficulty\",['Easy', 'Med', 'Hard'], [], None)\n",
    "E = BNNode(\"Effort\",['Low', 'Med', 'High'], [], None)\n",
    "A = BNNode(\"Aptitute\",[1,2,3,4,5], [], None)\n",
    "C = BNNode(\"Confidence\",['Low', 'Med', 'High'], [], None)\n",
    "G = BNNode(\"Grade\",['F', 'P', 'Cr', 'D', 'HD'], [D,E,A], None)\n",
    "I = BNNode(\"Interview\",['Bad', 'Ok', 'Godd'], [E,A,C], None)\n",
    "T = BNNode(\"TuteAttendence\",['Low', 'Med', 'High'], [E,C], None)\n",
    "F = BNNode(\"ForumParticipation\",['Low', 'Med', 'High'], [E,A,C], None)\n",
    "S = BNNode(\"SAT\",['Low', 'Med', 'High'], [A], None)\n",
    "J = BNNode(\"Job\",['No', 'Yes'], [G,I], None)\n",
    "\n",
    "nodes = [D, E, A, C, G, I, T, F, S, J]\n",
    "print(vars2names(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Part 1: General Properties\n",
    "### Question 1.1 [4 marks]\n",
    "\n",
    "1. Draw the Bayesian Network structure for the network defined above (note the structure is defined by the parents of the node specified when declaring each node).\n",
    "2. Write down the factorisation of the joint distribution given this structure.\n",
    "3. Calculate the number of parameters needed to represent the total joint distribution with this factorisation and without any factorisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. ![](./submission_extras/Q.1.1.png)\n",
    "\n",
    "\n",
    "2. $P(D, E, A, C, G, I, T, F, S, J) = P(D) P(E) P(A) P(C) P(G|D,E,A) P(I|A,E,C) P(F|A,E,C) P(T|E,C) P(J|G,I) P(S|A)$\n",
    "\n",
    "\n",
    "3. Without factorisation, the joint distribution would require $5^2 \\times 3^7 \\times 2 - 1 = 109,349$ parameters in total. With factorisation:\n",
    "    - $p(D)$ requires 2.\n",
    "    - $p(E)$ requires 2.\n",
    "    - $p(A)$ requires 4.\n",
    "    - $p(C)$ requires 2.\n",
    "    - $p(G|D, E, A)$ requires $(5 \\times 5)-5$ parameters for each pair of (D,E). Since there are 9 pairs of (D,E), $p(G|D, E, A)$ requires a total of $20 \\times 9 = 180$ parameters. \n",
    "    - $p(I|A, E, C)$ requires $(3 \\times 3)-3$ parameters for each pair of (A,E). Since there are 15 pairs of (A,E), $p(I|A,E,C)$ requires a total of $6 \\times 15 = 90$ parameters.\n",
    "    - $p(F|A,E,C)$ requires $(3\\times3)-3$ for each pair of (A,E). Since there are 15 pairs of (A,E), $p(F|A,E,C)$ requires a total of $6\\times15 = 90$ parameters.\n",
    "    - $p(T|E,C)$ requires $(3\\times3)-3$ parameters for every value of E. Since there are 3 E values, $p(T|E,C)$ requires a total of $6\\times3= 18$ parameters.\n",
    "    - $p(J|G,I)$ requires 3 parameters for every value of G. Since there are 5 G values, $p(J|G,I)$ requires a total of $3\\times5 = 15$ parameters.  \n",
    "    - $p(S|A)$ requires 2 parameters for every value of A. Since there are 5 A values, $p(S|A)$ requires a total of $2\\times5 = 10$ parameters.  \n",
    "    \n",
    "Hence, with factorisation, a total of $2+2+4+2+180+90+90+18+15+10 = 413$ parameters is needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 [6 marks]\n",
    "Do the following conditional indpendencies hold? Prove your answer.\n",
    "\n",
    "1. $C \\perp \\!\\!\\! \\perp G\\mid I$\n",
    "2. $C \\perp \\!\\!\\! \\perp G\\mid E$\n",
    "3. $C \\perp \\!\\!\\! \\perp G\\mid I,E$\n",
    "4. $C \\perp \\!\\!\\! \\perp G\\mid I,E,J$\n",
    "5. $F \\perp \\!\\!\\! \\perp J\\mid I,G$\n",
    "6. $F \\perp \\!\\!\\! \\perp J\\mid I,E,A$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. False. The path {C, I, A, G} is not blocked because I is an onbserved HH-node. \n",
    "2. True. If we choose any path from C to G that passes through E, E will act as an TT-node. If a path does not pass through E, it has to visit either I, F, or J, which will act as an unobserved HH-node with their decendants also unobserved.\n",
    "3. False. The path {C, I, A, G} is not blocked because I is an observed HH-node. \n",
    "4. False. The path {C, I, A, G} is not blocked because I is an observed HH-node and its decendent J is also observed.\n",
    "5. True. To get from F to J, we need to pass either I or G or both of I and G. In all three cases, I/G is an observed HT-node. \n",
    "6. True. From F, we can reach E, A, and C\n",
    "    - Any path continues from E will have E act as an observed TT-node. Same argument can be made for any path continues from A. \n",
    "    - Paths continue from C will belong to one of these 2 cases:\n",
    "        1. Goes to I and straight to J => I will act as an observed HT-node athus the path is blocked.\n",
    "        2. Goes to I and back to A or C => A or C will act as an observed TT-node similar to above and the path is blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Part 2: Learning the Parameters from Data\n",
    "\n",
    "We now want to learn the parameters of the model given observed samples. However we will only have observed samples for some of the variables, specifically all the variables except `Aptitude`. Thus $A$ is a latent variable, and all other variables are observed. Furthermore we will have some parameters of the model, specifically $p(S\\mid A)$ and $p(F\\mid E,A,C)$. We can think of this as putting in information from other sources: we have found a model (maybe from previous research) for how SAT scores $S$ are affected by Aptitude and how Forum Participation $F$ are affected by Effort, Aptitude and Confidence, so we put this information in directly.\n",
    "\n",
    "However as we have a latent variable, we won't be able to solve directly for all the other unknown parameters using Maximum Likelihood Estimation (MLE). So instead we will need to use Expectation Maximisation (EM) to be able to learn the parameters.\n",
    "\n",
    "We first initialise the (other) pmfs to uniform probability, and load in observed samples. The format for the sample data, `sample_idxes`, is a $n\\times d$ array where $n$ is the number of samples (5000) and $d$ is the number of nodes (10). The value in each column is the index of the observed state for that variable, and the columns are in the same order as `nodes`.\n",
    "\n",
    "As `A` is not observed, the indices for that column are all -1.\n",
    "Note that the observed values are encoded as 0:K-1, where K is the number of states in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# Initialise parameters to uniform\n",
    "D.pmf = PMF([D], np.ones((3,))/3)\n",
    "E.pmf = PMF([E], np.ones((3,))/3)\n",
    "A.pmf = PMF([A], np.ones((5,))/5)\n",
    "C.pmf = PMF([C], np.ones((3,))/3)\n",
    "\n",
    "G.pmf = PMF([D,E,A,G], np.ones((3,3,5,5))/(5))\n",
    "I.pmf = PMF([E,A,C,I], np.ones((3,5,3,3))/(3))\n",
    "T.pmf = PMF([E,C,T], np.ones((3,3,3))/(3))\n",
    "F.pmf = PMF([E,A,C,F], np.ones((3,5,3,3))/(3))\n",
    "S.pmf = PMF([A,S], np.ones((5,3))/(3))\n",
    "J.pmf = PMF([G,I,J], np.ones((5,3,2))/(2))\n",
    "\n",
    "# Load and set parameters for specific variables\n",
    "S_pmf = np.load(os.path.join(\"data\",\"question_1\",\"S_pmf.npy\"))\n",
    "S.pmf.values = S_pmf\n",
    "F_pmf = np.load(os.path.join(\"data\",\"question_1\",\"F_pmf.npy\"))\n",
    "F.pmf.values = F_pmf\n",
    "\n",
    "for var in nodes:\n",
    "    var.check_pmf(var.pmf) # Make sure the pmfs are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7  0.25 0.05]\n",
      " [0.6  0.3  0.1 ]\n",
      " [0.4  0.5  0.1 ]\n",
      " [0.2  0.6  0.2 ]\n",
      " [0.05 0.45 0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# examine one of the loaded pmfs P(S|A)\n",
    "print(S.pmf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10)\n",
      "[[ 1  1 -1  1  3  1  2  2  2  0]\n",
      " [ 1  0 -1  1  0  0  0  0  0  0]\n",
      " [ 0  2 -1  2  4  1  2  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# You're given a dataset consisting of records of 5000 students\n",
    "# load this, and examine the first three rows\n",
    "sample_idxes = np.load(os.path.join(\"data\",\"question_1\",\"sample_idxes_data.npy\"))\n",
    "print(sample_idxes.shape)\n",
    "print(sample_idxes[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 [10 Marks]\n",
    "We can now learn the parameters by maximum likelihood estimation for some of the variables. \n",
    "Let $X$ denote the observed variables, $X=[D,E,C,G,I,T,F,S,J]$, and $Z$ denote the unobserved variables, $Z=[A]$. We can denote assignments to the variables by $\\alpha=\\{x_n,z_n\\}_{n=1}^N$ where $x_n$ are the observed assignments for the $n^\\text{th}$ sample and $z_n$ are the unobserved assignments. So $x_n^{(D,d)}=1$ if in the $n^\\text{th}$ sample the value of $D$ was state $d$, and $x_n^{(D,d)}=0$ otherwise. Likewise the unknown assignments for the latent variables are denoted $z_n^{(A,a)}\\in{0,1}$. \n",
    "\n",
    "Also let us represent the parameters of a variable of the model, which represent the conditional probability for that variable w.r.t. its parents $p(\\text{var} \\mid \\text{pa(var)})$, by $\\theta^{var}$. For example, \n",
    "$$p(G=g \\mid D=d,A=a,C=c)=\\theta^{(G)}_{d,a,c,g}.$$\n",
    "\n",
    "Then we can represent the joint probability in this way:\n",
    "\\begin{align*}\n",
    "p(X,Z|\\alpha,\\theta)\n",
    "    &=\\prod_{n}\\left(p(D|x_n,z_n,\\theta^{(D)}) ... p(G|x_n,z_n,D,A,C,\\theta^{(G)}) ...\\right)\\\\\n",
    "    &=\\prod_{n}\\left(\\left(\\prod_{d\\in D}(\\theta^{(D)}_d)^{x_n^{(D,d)}}\\right) ... \\left(\\prod_{d\\in D}\\prod_{a\\in A}\\prod_{c\\in C}\\prod_{g\\in G}(\\theta^{(G)}_{d,a,c,g})^{x_n^{(D,d)}z_n^{(A,a)}x_n^{(C,c)}x_n^{(G,g)}}\\right) ... \\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note we have the values $x_n$ but do not have the values $z_n$.\n",
    "\n",
    "1. Which variables can we derive the MLE solution for their parameters for (note this would mean representing their parameters in terms of $x_n$ only, without $z_n$). Explain why.\n",
    "\n",
    "2. (M-step for easy variables)\n",
    "Show that the MLE solution for $\\theta^{(D)}$ is given by\n",
    "$$\\theta^{(D)}_d = \\frac{\\sum_n x_n^{(D,d)}}{\\sum_{d'}\\sum_n x_n^{(D,d')}}$$\n",
    "and argue that we can generalise to a general MLE solution in terms of a variable $V$ and its ancestors $U_1,...,U_n$\n",
    "$$\\theta^{(V)}_{{u_1}...{u_n}v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v)}}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...x_n^{(U_n,{u_n})}x_n^{(V,v')}\\right)}$$\n",
    "\n",
    " (Hint: maximise the log of the joint probability, and use Lagrange Multipliers to enforce that the sum of conditionals should sum to 1).\n",
    "\n",
    "3. Code up the general MLE solution and use it to calculate the parameters for the variables you listed in part 1. Update the pmfs of the nodes made in Q1/2 have these parameters. For the $x_n$'s we suggest you use the assignments array we make below, and fit the rest of the solution using the template we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1.3.1. We can derive the MLE solution for D, E, C, and T as they are not A or dependent on A, thus their parameters can be represented only in terms of $x_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# Intialise assignments\n",
    "n, v = sample_idxes.shape\n",
    "assignments = np.zeros((n, v, 5)) # 5 as that is the maximum number of states of any variable\n",
    "for i in range(n):\n",
    "    for j in range(v):\n",
    "        assignments[i,j,sample_idxes[i,j]] = 1\n",
    "assignments[:,nodes.index(A)] = np.nan # No assignments as it (A) is latent latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parameters(curr_var, assignments):\n",
    "    # Calculate the shape of the pmf array for the cur_var\n",
    "    pmf_shape = tuple(len(var.states) for var in curr_var.pmf.dims)\n",
    "    # Initialise a pmf array of the shape calculated above with 0 values\n",
    "    var_pmf = np.zeros(pmf_shape)\n",
    "    \n",
    "    idx = nodes.index(curr_var)\n",
    "        \n",
    "    if len(curr_var.parents) == 0:\n",
    "        for i in range(pmf_shape[0]):\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            for n in range(len(assignments)):\n",
    "                #print(sample_idxes[n])\n",
    "                row_var = assignments[n][idx]\n",
    "                #print(row_var)\n",
    "                numerator += row_var[i]\n",
    "                denominator += np.sum(row_var)\n",
    "            var_pmf[i] = numerator/denominator\n",
    "\n",
    "    else:\n",
    "        states = []\n",
    "        pa_idx = [nodes.index(pa) for pa in curr_var.parents]\n",
    "        for pa in curr_var.parents:\n",
    "            s = [i for i in range(len(pa.states))]\n",
    "            states.append(s)\n",
    "\n",
    "        parent_states_combo = list(itertools.product(*states))\n",
    "                \n",
    "        for state_combo in parent_states_combo:\n",
    "            \n",
    "            for i in range(len(curr_var.states)):\n",
    "                numerator = 0\n",
    "                denominator = 0\n",
    "                \n",
    "                for n in range(len(assignments)):\n",
    "                    n_assignment = assignments[n]\n",
    "                    row_var = n_assignment[idx]\n",
    "                    \n",
    "                    pa_assignment = n_assignment[pa_idx]\n",
    "                    \n",
    "                    numerator_ls = [row_var[i]]\n",
    "                    denominator_ls = [np.sum(row_var)]\n",
    "                    \n",
    "                    for k in range(len(state_combo)):\n",
    "                        #print(k, pa_assignment[k], pa_assignment[k][state_combo[k]])\n",
    "                        numerator_ls.append(pa_assignment[k][state_combo[k]])\n",
    "                        denominator_ls.append(pa_assignment[k][state_combo[k]])\n",
    "\n",
    "    \n",
    "                    numerator += np.prod(numerator_ls)\n",
    "                    denominator += np.prod(denominator_ls)\n",
    "            \n",
    "                res = numerator/denominator\n",
    "                idex = [val for val in state_combo]\n",
    "                idex.append(i)\n",
    "                var_pmf[tuple(idex)] = res\n",
    "\n",
    "\n",
    "    # TODO: Use assignments to calculate the values for the pmf array\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    # assign the newly calculated pmf values to the curr_var's pmf array\n",
    "    curr_var.pmf.values = var_pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2058, 0.4964, 0.2978])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# list the nodes that we can derive the MLE solution for\n",
    "mle_nodes = [D, E, C, T] # TODO: fill this in\n",
    "try:\n",
    "    for node in mle_nodes:\n",
    "        calculate_parameters(node, assignments)    \n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement calculate_parameters\")\n",
    "if len(mle_nodes) == 0:\n",
    "    print(\"Need to add nodes to mle_nodes\")\n",
    "D.pmf.values # Should be roughly [0.2,0.5,0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 [10 Marks]\n",
    "\n",
    "We now turn to how to properly train our latent variable model using the Expectation Maximization algorithm. Feel free to refer to Section 9.3 and Section 9.4 of the Bishop book for a refresher of this content. \n",
    "\n",
    "The objective of EM is to maximise the expectation of the complete log-likelihood w.r.t. the latent variables:\n",
    "\\begin{align*}\n",
    "E_Z[\\log p(X,Z \\mid \\alpha,\\theta)]=\\sum_Zp(Z\\mid X, \\alpha, \\theta)\\log p(X,Z \\mid \\alpha,\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "In the E-step we should be calculate the expected value of assignments to the latent variables with respect to the data and the current parameters. Thus as $A$ is the only latent parameter, we should be able to calculate the expected value of $z_n^{(A,a)}$, which we can denote by $E\\left[z_n^{(A,a)}\\right]$.\n",
    "\n",
    "Furthermore, as the values of $z_n^{(A,a)}$ are 0 or 1, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E\\left[z_n^{(A,a)}\\right]\n",
    "    &=\\sum_{v\\in\\{0,1\\}} v*p\\left(z_n^{(A,a)}=v\\mid x_n,\\theta\\right)\\\\\n",
    "    &= p\\left(z_n^{(A,a)}=1\\mid x_n,\\theta\\right)\\\\\n",
    "    &= \\frac{p\\left(x_n,z_n^{(A,a)}=1\\mid \\theta\\right)}{p(x_n\\mid \\theta)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "1. (M-step for hard variables)  Given such expected values, we should be able to use MLE to solve for the parameters of all other variables, and get the exact same form as the general solution given in part 2. of the last question. More specifically given latent variable $V$ with observed ancestors $U_i$ and unobserved ancestors $W_i$, where all the latent variables are independent of each other (i.e. the $W_i$ and $V$ are all independent from each other), then the parameters for $V$ can be given by \n",
    "$$\\theta^{(V)}_{{u_1}...{w_1}...v} = \\frac{\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v)}\\right]}{\\sum_{v'}\\left(\\sum_n x_n^{(U_1,{u_1})}...E\\left[z_n^{(W_1,{w_1})}\\right]...E\\left[z_n^{(V,v')}\\right]\\right)}.$$\n",
    "Show this by solving for the parameters that maximise the EM objective (and again use Lagrange Multipliers, the steps should be very similar).\n",
    "\n",
    "2. (E-step) Note that the equation given in 1. can be implemented using the exact same function we wrote in Q1 (`calculate_parameters`) where the values used in `assignments` are the expectations $E\\left[z_n^{(A,a)}\\right]$. However it is also equivalent to having just the numerator probabilities, $p\\left(x_n,z_n^{(A,a)}=1\\mid \\theta\\right)$, as the value in the assignments, i.e. `assignments[n,nodes.index(A),A.states.index(a)]`=$p\\left(x_n,z_n^{(A,a)}=1\\mid \\theta\\right)$. Show that this is the case (i.e. that the denominators cancel out) and implement getting such probabilities. Use our template to do this, which will work in the log domain as otherwise we would be multiplying lots of small probabilities which leads to numerical instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    " {Type your solution to 1.4.1 and the \"show\" part of 1.4.2 here}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ll(nodes, single_sample_idxes):\n",
    "    # Assume that nodes is in the topological order, and single_sample_idxes has the state\n",
    "    # index of each variable (including latent variable) for a one instance.\n",
    "    single_sample_idxes = np.array(single_sample_idxes).reshape(-1)\n",
    "    \n",
    "    assert len(nodes) == len(single_sample_idxes)\n",
    "    ll = 0\n",
    "    \n",
    "    # TODO: Sum up the log probs of each \n",
    "    raise NotImplementedError\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to implement sample_ll\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "try:\n",
    "    # Initialise the assignments to 0\n",
    "    intel_assignments = np.zeros((len(sample_idxes), 5))\n",
    "    for i in range(len(sample_idxes)):\n",
    "        instance_probs = []\n",
    "        for state in range(5):\n",
    "            data = np.array(sample_idxes[i])\n",
    "            data[2] = state # Set Intel's value to state\n",
    "            ll = sample_ll(nodes, data)\n",
    "            instance_probs.append(ll)\n",
    "        instance_probs = np.exp(np.array(instance_probs))\n",
    "        intel_assignments[i] = instance_probs/instance_probs.sum() # normalise ourselves\n",
    "    print(intel_assignments[0]) # Should be approx [0.01,0.02,0.03,0.14,0.80]\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement sample_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the EM algorithm\n",
    "Now we can run our EM algorithm to completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial A marginals:  [0.2 0.2 0.2 0.2 0.2]\n",
      "Need to implement sample_ll!\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "try:\n",
    "    print(\"Initial A marginals: \", A.pmf.values)\n",
    "    old_norm_diff = 1\n",
    "    for it in range(100):\n",
    "        # E step\n",
    "        apt_assignments = np.zeros((len(sample_idxes), 5))\n",
    "        for i in range(len(sample_idxes)):\n",
    "            instance_probs = []\n",
    "            for state in range(5):\n",
    "                data = np.array(sample_idxes[i])\n",
    "                data[2] = state\n",
    "                ll = sample_ll(nodes, data)\n",
    "                instance_probs.append(ll)\n",
    "            instance_probs = np.exp(np.array(instance_probs))\n",
    "            apt_assignments[i] = instance_probs/instance_probs.sum()\n",
    "        # Set assignments made in E-step\n",
    "        assignments[:,2,:] = apt_assignments\n",
    "        # M step\n",
    "        old_A_pmfs = np.array(A.pmf.values)\n",
    "        for curr_var in nodes:\n",
    "            if curr_var in [S, F]: # Do not override the parameters we loaded in\n",
    "                continue\n",
    "            ind = [nodes.index(var) for var in curr_var.pmf.dims] # [0, 1, 2, 4]\n",
    "            pmf_shape = tuple(len(var.states) for var in curr_var.pmf.dims) # (3,3,5,5)\n",
    "            var_pmf = np.zeros(pmf_shape)\n",
    "            for values in itertools.product(*(range(len(var.states)) for var in curr_var.pmf.dims)):\n",
    "                var_pmf[values] = assignments[:,ind,values].prod(axis=1).sum()\n",
    "            var_pmf /= var_pmf.sum(axis=-1, keepdims=True)\n",
    "            curr_var.pmf.values = var_pmf\n",
    "        # Check for convergence\n",
    "        norm_diff = np.linalg.norm(old_A_pmfs-A.pmf.values)\n",
    "        if (old_norm_diff-norm_diff)/old_norm_diff < 0.005:\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "        old_norm_diff = norm_diff\n",
    "        print(\"Iteration {}, A marginals: {}, Norm Diff {:.5f}\".format(it, A.pmf.values, norm_diff)) \n",
    "        # Should get to roughly [0.05, 0.2, 0.35, 0.3, 0.1], something like [0.08, 0.18, 0.36, 0.27, 0.11]\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement sample_ll!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN Part 3: Inference with the learned model\n",
    "\n",
    "If you have not finished Part 2 and don't have a trained model, load in the following parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "part2complete = False # Change this if you have not completed part 2!!!\n",
    "if not part2complete:\n",
    "    with open(os.path.join(\"data\",\"question_1\",\"saved_pmf_vals.pkl\"), 'rb') as fp:\n",
    "        saved_pmf_vals = pickle.load(fp)\n",
    "\n",
    "    for i, var in enumerate(nodes):\n",
    "        var.pmf.values = saved_pmf_vals[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5 [10 Marks]\n",
    "1. Calculate the following marginals/conditionals using the `sample_idxes` data: $p(J)$, $p(G)$, $p(I)$, $p(J|G)$, $p(J|C)$, $p(J|E)$.\n",
    "\n",
    "2. Implement a function that marginalises over variables. That is, given variables to keep, it sums over all other variables. For example, given \n",
    "`marginaliseConditionals(nodes, [C])` it will sum over all other variables except for $C$. Thus in $C$ it would leave $p(C)$, in $I$ it would leave $p(I|C)$, in $A$ it would leave $p(A)$ as it doesn't have $C$ as an acestor, and in $J$ it would leave $p(J|C)$. Use this to calculate $p(J|C)$, $p(J|A)$, $p(J|E)$ and $p(J|E,A)$. Check your code by comparing to the calculations done above (they should only have slightly differing values). Furthermore calculate $p(J|A)$, which we couldn't do with the sample_idxes.\n",
    "\n",
    " (Hint: use the `multiply_pmf` function provided).\n",
    "\n",
    "3. Calculate $p(E|J)$ (`marginaliseConditionals` should be useful here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5.1: Calculate the marginals/conditionals from the data and print them out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(J): [0.8736 0.1264]\n"
     ]
    }
   ],
   "source": [
    "# P(J)\n",
    "\n",
    "J_prob = np.zeros(len(J.states))\n",
    "\n",
    "for i in range(len(J.states)):\n",
    "    p = np.sum(sample_idxes[:,nodes.index(J)] == i)\n",
    "    total = len(sample_idxes)\n",
    "    J_prob[i] = p/total\n",
    "    \n",
    "print('P(J):', J_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(G): [0.2044 0.1786 0.1968 0.185  0.2352]\n"
     ]
    }
   ],
   "source": [
    "# P(G)\n",
    "\n",
    "G_prob = np.zeros(len(G.states))\n",
    "\n",
    "for i in range(len(G.states)):\n",
    "    p = np.sum(sample_idxes[:,nodes.index(G)] == i)\n",
    "    total = len(sample_idxes)\n",
    "    G_prob[i] = p/total\n",
    "    \n",
    "print('P(G):', G_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(I): [0.4616 0.4314 0.107 ]\n"
     ]
    }
   ],
   "source": [
    "# P(I)\n",
    "\n",
    "I_prob = np.zeros(len(I.states))\n",
    "\n",
    "for i in range(len(I.states)):\n",
    "    p = np.sum(sample_idxes[:,nodes.index(I)] == i)\n",
    "    total = len(sample_idxes)\n",
    "    I_prob[i] = p/total\n",
    "    \n",
    "print('P(I):', I_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(J|G):\n",
      " [[0.95303327 0.04696673]\n",
      " [0.95968645 0.04031355]\n",
      " [0.94817073 0.05182927]\n",
      " [0.88432432 0.11567568]\n",
      " [0.66836735 0.33163265]]\n"
     ]
    }
   ],
   "source": [
    "# P(J|G)\n",
    "\n",
    "JgivenG_prob = np.zeros((len(G.states), len(J.states)))\n",
    "\n",
    "for i in range(len(G.states)):\n",
    "    Gi_prob = G_prob[i]\n",
    "    for j in range(len(J.states)):\n",
    "        joint_total = 0\n",
    "        for row in sample_idxes:\n",
    "            if row[nodes.index(G)] == i and row[nodes.index(J)] == j:\n",
    "                joint_total += 1\n",
    "        p_joint = joint_total / len(sample_idxes)\n",
    "        JgivenG_prob[i,j] = p_joint/Gi_prob\n",
    "    \n",
    "\n",
    "print('P(J|G):\\n', JgivenG_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(C): [0.211 0.585 0.204]\n",
      "\n",
      "P(J|C):\n",
      " [[0.88341232 0.11658768]\n",
      " [0.87316239 0.12683761]\n",
      " [0.86470588 0.13529412]]\n"
     ]
    }
   ],
   "source": [
    "# P(J|C)\n",
    "\n",
    "C_prob = np.zeros(len(C.states))\n",
    "\n",
    "for i in range(len(C.states)):\n",
    "    p = np.sum(sample_idxes[:,nodes.index(C)] == i)\n",
    "    total = len(sample_idxes)\n",
    "    C_prob[i] = p/total\n",
    "    \n",
    "print('P(C):', C_prob)\n",
    "\n",
    "JgivenC_prob = np.zeros((len(C.states), len(J.states)))\n",
    "\n",
    "for i in range(len(C.states)):\n",
    "    Ci_prob = C_prob[i]\n",
    "    for j in range(len(J.states)):\n",
    "        joint_total = 0\n",
    "        for row in sample_idxes:\n",
    "            if row[nodes.index(C)] == i and row[nodes.index(J)] == j:\n",
    "                joint_total += 1\n",
    "        p_joint = joint_total / len(sample_idxes)\n",
    "        JgivenC_prob[i,j] = p_joint/Ci_prob\n",
    "\n",
    "print()\n",
    "print('P(J|C):\\n', JgivenC_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(E): [0.3962 0.392  0.2118]\n",
      "\n",
      "P(J|E):\n",
      " [[0.92983342 0.07016658]\n",
      " [0.87142857 0.12857143]\n",
      " [0.77242682 0.22757318]]\n"
     ]
    }
   ],
   "source": [
    "# P(J|C)\n",
    "\n",
    "E_prob = np.zeros(len(E.states))\n",
    "\n",
    "for i in range(len(E.states)):\n",
    "    p = np.sum(sample_idxes[:,nodes.index(E)] == i)\n",
    "    total = len(sample_idxes)\n",
    "    E_prob[i] = p/total\n",
    "    \n",
    "print('P(E):', E_prob)\n",
    "\n",
    "JgivenE_prob = np.zeros((len(E.states), len(J.states)))\n",
    "\n",
    "for i in range(len(E.states)):\n",
    "    Ei_prob = E_prob[i]\n",
    "    for j in range(len(J.states)):\n",
    "        joint_total = 0\n",
    "        for row in sample_idxes:\n",
    "            if row[nodes.index(E)] == i and row[nodes.index(J)] == j:\n",
    "                joint_total += 1\n",
    "        p_joint = joint_total / len(sample_idxes)\n",
    "        JgivenE_prob[i,j] = p_joint/Ei_prob\n",
    "    \n",
    "print()\n",
    "print('P(J|E):\\n', JgivenE_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "def multiply_pmfs(pmf1,pmf2,var):\n",
    "    # Implements marginalising pmf2 over var using pmf1. pmf1 is not modified, but pmf2 is.\n",
    "    # Assumes the following dimensions\n",
    "    # pmf1.dims : (a1,....,an,var)\n",
    "    # pmf2.dims : (b1,.....,bm,var2) where var (and maybe some of a1,...,an) is contained in b1,...,bm\n",
    "\n",
    "    # Make a copy\n",
    "    pmf1 = PMF(list(pmf1.dims), np.array(pmf1.values))\n",
    "    assert var == pmf1.dims[-1], (var.name, vars2names(pmf1.dims))\n",
    "    assert var in pmf2.dims, (var.name, vars2names(pmf2.dims))\n",
    "    a_is = pmf1.dims[:-1]\n",
    "    b_is = pmf2.dims[:-1]\n",
    "    assert var in b_is, (var.name,vars2names(b_is))\n",
    "    b_is.remove(var)\n",
    "\n",
    "    intersect = set(a_is) & set(b_is)\n",
    "\n",
    "    i = pmf2.dims.index(var)\n",
    "    pmf2.values = np.moveaxis(pmf2.values, i, -2)\n",
    "    pmf2.dims = pmf2.dims[:i] + pmf2.dims[i+1:] # Remove var\n",
    "    # Now pmf2: ({b1,...,bm}\\var,var,var2)\n",
    "    pmf1.values = np.expand_dims(pmf1.values,axis=-2)\n",
    "    # Now pmf2: ({a1,...,bn},1,var)\n",
    "    # print(\"Intersect\", vars2names(intersect))\n",
    "    for var_i in intersect:\n",
    "        i = pmf1.dims.index(var_i)\n",
    "        pmf1.values = np.moveaxis(pmf1.values, i, -3)\n",
    "        pmf1.dims = pmf1.dims[:i] + pmf1.dims[i+1:-1] + [var_i] + pmf1.dims[-1:]\n",
    "        i = pmf2.dims.index(var_i)\n",
    "        pmf2.values = np.moveaxis(pmf2.values, i, -3)\n",
    "        # print(pmf2.dims[:i] , pmf2.dims[i+1:-2] , [var_i] , pmf2.dims[-2:])\n",
    "        pmf2.dims = pmf2.dims[:i] + pmf2.dims[i+1:-1] + [var_i] + pmf2.dims[-1:]\n",
    "    # Now pmf2: ({b1,...,bm}\\{var}\\cup {intersect},{intersect},var,var2)\n",
    "    # Now pmf1: ({11,...,an}\\{intersect},{intersect},1,var2)\n",
    "    # print('1:', pmf1, pmf2)\n",
    "    pmf2_other = set(b_is) - intersect\n",
    "    pmf1_other = set(a_is) - intersect\n",
    "    for _ in range(len(pmf2_other)):\n",
    "        pmf1.values = np.expand_dims(pmf1.values,axis=len(pmf1_other))\n",
    "    pmf2.dims = pmf1.dims[:len(pmf1_other)] + pmf2.dims\n",
    "    # print('2:', pmf1, pmf2)\n",
    "    # Now pmf2: ({b1,...,bm}\\{var}\\cup {intersect},{intersect},var,var2)\n",
    "    # Now pmf1: ({11,...,an}\\{intersect},{1,...,1},{intersect},1,var2)\n",
    "    # print(\"Shapes\", pmf1.values.shape, pmf2.values.shape)\n",
    "    pmf2.values = pmf1.values @ pmf2.values\n",
    "    # Now ({11,...,an}\\{intersect},{b1,...,bm}\\{var}\\cup {intersect},{intersect},1,var2)\n",
    "    pmf2.values = np.squeeze(pmf2.values, axis=-2)\n",
    "    # Now ({11,...,an}\\{intersect},{b1,...,bm}\\{var}\\cup {intersect},{intersect},var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginaliseConditionals(top_order, rem_vars):\n",
    "    # rem_vars are the vars to not marginalise over\n",
    "    \n",
    "    #print(rem_vars)\n",
    "    for var in top_order:\n",
    "        # Make a copy\n",
    "        new_pmf = PMF(list(var.pmf.dims), np.array(var.pmf.values))\n",
    "        \n",
    "        # TODO: Implement marginalising over all variables in var.pmf.dims apart from any vars in rem_var\n",
    "        # Hint: use multiply_pmfs\n",
    "        \n",
    "        # Create a copy of all pmfs to work with \n",
    "        pmf_dict = {}\n",
    "        for node in top_order:\n",
    "            pmf_dict[node] = PMF(list(node.pmf.dims), np.array(node.pmf.values))\n",
    "        \n",
    "        for par in var.parents:\n",
    "            if par not in rem_vars:\n",
    "                for par_par in par.parents:\n",
    "                    # This is for the case where any of the rem_vars is not immediately related to var\n",
    "                    # e.g. p(J|C), since J is conditioned on I and I is conditioned on E, A, C\n",
    "                    # we will marginalise I over E and A -> p(I|C), then marginalise J over the new pmf of I\n",
    "                    if par_par not in rem_vars:\n",
    "                        multiply_pmfs(pmf_dict[par_par], pmf_dict[par], par_par)\n",
    "                            \n",
    "                multiply_pmfs(pmf_dict[par], new_pmf, par)\n",
    "        \n",
    "        var.new_pmf = new_pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginals for Job\n",
      "[0.88233116 0.11766884]\n",
      "\n",
      "Marginals for Grade\n",
      "[0.20291146 0.17875817 0.19789516 0.18499422 0.23544098]\n",
      "\n",
      "Marginals for Interview\n",
      "[0.46217175 0.430716   0.10711225]\n",
      "\n",
      "Marginals for Job given Grade\n",
      "[[0.95010592 0.04989408]\n",
      " [0.95986037 0.04013963]\n",
      " [0.94654524 0.05345476]\n",
      " [0.88732419 0.11267581]\n",
      " [0.70715949 0.29284051]]\n",
      "\n",
      "Marginals for Job given Confidence\n",
      "[[0.89808225 0.10191775]\n",
      " [0.88363667 0.11636333]\n",
      " [0.86229587 0.13770413]]\n",
      "\n",
      "Marginals for Job given Effort\n",
      "[[0.93234492 0.06765508]\n",
      " [0.8774997  0.1225003 ]\n",
      " [0.77154083 0.22845917]]\n",
      "\n",
      "Marginals for Job given Confidence\n",
      "[[0.89808225 0.10191775]\n",
      " [0.88363667 0.11636333]\n",
      " [0.86229587 0.13770413]]\n",
      "\n",
      "Marginals for Job given Aptitute\n",
      "[[0.92812449 0.07187551]\n",
      " [0.91948149 0.08051851]\n",
      " [0.88735288 0.11264712]\n",
      " [0.85253003 0.14746997]\n",
      " [0.81464202 0.18535798]]\n",
      "\n",
      "Marginals for Job given Effort\n",
      "[[0.93234492 0.06765508]\n",
      " [0.8774997  0.1225003 ]\n",
      " [0.77154083 0.22845917]]\n",
      "\n",
      "Marginals for Job given Effort, Aptitute\n",
      "Dims:  ['Aptitute', 'Effort', 'Job']\n",
      "[[[0.9389722  0.0610278 ]\n",
      "  [0.92979817 0.07020183]\n",
      "  [0.90532418 0.09467582]]\n",
      "\n",
      " [[0.95046441 0.04953559]\n",
      "  [0.94069482 0.05930518]\n",
      "  [0.76274913 0.23725087]]\n",
      "\n",
      " [[0.94543455 0.05456545]\n",
      "  [0.88317502 0.11682498]\n",
      "  [0.75662671 0.24337329]]\n",
      "\n",
      " [[0.91379264 0.08620736]\n",
      "  [0.81820705 0.18179295]\n",
      "  [0.78478925 0.21521075]]\n",
      "\n",
      " [[0.88003755 0.11996245]\n",
      "  [0.80574967 0.19425033]\n",
      "  [0.70308651 0.29691349]]]\n"
     ]
    }
   ],
   "source": [
    "## Supporting Cell Do NOT change\n",
    "\n",
    "# 1.5.2: Use marginaliseConditionals to calculate the marginals/conditionals from the trained BN and print them out here\n",
    "# Check against prev values calculated from the data (if possible)\n",
    "try:\n",
    "    for var in [J, G, I]:\n",
    "        marginaliseConditionals(nodes, [var])\n",
    "        print(\"Marginals for {}\".format(var.name))\n",
    "        print(var.new_pmf.values)\n",
    "        print()\n",
    "    for var1, var2 in [(J, G), (J, C), (J, E)]:\n",
    "        marginaliseConditionals(nodes, [var2])\n",
    "        print(\"Marginals for {} given {}\".format(var1.name, var2.name))\n",
    "        print(var1.new_pmf.values)\n",
    "        print()\n",
    "    for var1, var2 in [(J, C), (J, A), (J, E)]:\n",
    "        marginaliseConditionals(nodes, [var2])\n",
    "        print(\"Marginals for {} given {}\".format(var1.name, var2.name))\n",
    "        print(var1.new_pmf.values)\n",
    "        print()\n",
    "    marginaliseConditionals(nodes, [E, A])\n",
    "    print(\"Marginals for {} given {}, {}\".format(J.name, E.name, A.name))\n",
    "    print(\"Dims: \", vars2names(J.new_pmf.dims))\n",
    "    print(J.new_pmf.values)\n",
    "except NotImplementedError:\n",
    "    print(\"Need to implement marginaliseConditionals!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5.1: Calculate p(E|J) here. Should use marginaliseConditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginals for Effort given Job\n",
      "[0.3962 0.392  0.2118]\n"
     ]
    }
   ],
   "source": [
    "marginaliseConditionals(nodes, [J])\n",
    "print(\"Marginals for {} given {}\".format(E.name, J.name))\n",
    "print(E.new_pmf.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is expected since Effort does not have Job as an ancestor. Therefore $p(E|J) = p(E)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
